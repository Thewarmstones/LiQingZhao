{
    "type": "error",
    "data": "Error Invoking LLM",
    "stack": "Traceback (most recent call last):\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n    return await delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 166, in _execute_llm\n    completion = await self._client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1727, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1849, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1543, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1582, in _request\n    response = await self._client.send(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\httpx\\_client.py\", line 1629, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\httpx\\_client.py\", line 1657, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\httpx\\_client.py\", line 1694, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\httpx\\_client.py\", line 1730, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 394, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 256, in handle_async_request\n    raise exc from None\n  File \"D:\\Miniconda3\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 236, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\httpcore\\_async\\http_proxy.py\", line 343, in handle_async_request\n    return await self._connection.handle_async_request(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 136, in handle_async_request\n    raise exc\n  File \"D:\\Miniconda3\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 106, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 177, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 217, in _receive_event\n    data = await self._network_stream.read(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 35, in read\n    return await self._stream.receive(max_bytes=max_bytes)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\anyio\\streams\\tls.py\", line 204, in receive\n    data = await self._call_sslobject_method(self._ssl_object.read, max_bytes)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\anyio\\streams\\tls.py\", line 147, in _call_sslobject_method\n    data = await self.transport_stream.receive()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1246, in receive\n    await self._protocol.read_event.wait()\n  File \"D:\\Miniconda3\\Lib\\asyncio\\locks.py\", line 212, in wait\n    await fut\nasyncio.exceptions.CancelledError\n",
    "source": "",
    "details": {
        "prompt": "-目标-\n给定一个可能与此活动相关的文本文档和实体类型列表，识别出文本中所有这些类型的实体以及识别出的实体之间的所有关系。文本内容是关于 李清照 的生平事迹。\n\n-步骤-\n1.识别所有实体。对于每个识别出的实体，提取以下信息：\nentity_name: 实体名称，首字母大写\nentity_type: 以下类型之一：[ORGANIZATION,PERSON,GEO,EVENT,DATE]\nentity_description: 实体属性和活动的详细描述\n将每个实体格式化为 (\"entity\"<|><entity_name><|><entity_type><|><entity_description>)\n\n2.从步骤1中识别出的实体中，找出所有明显相关 的（source_entity, target_entity）对。\n对于每对相关的实体，提取以下信息：\nsource_entity: 源实体的名称，如步骤1中所识别\ntarget_entity: 目标实体的名称，如步骤1中所识别\nrelationship_description: 认为源实体和目标实体相关的解释\nrelationship_strength: 表示源实体与目标实体之间关系强度的数字评分\n将每个关系格式化为 (\"relationship\"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n\n3.将步骤1和步骤2中识别出的所有实体和关系以英文返回为单个列表。使用 ## 作为列表分隔符。\n\n4.完成后，输出 <|COMPLETE|>\n\n######################\n-案例-\n######################\n案例 1:\nEntity_types: GEO, PERSON, DATE\nText:\n开封（1126年5月，43岁）\n1126年，43岁\n1126年5月　夏，夫妻共赏白居易所书《楞严经》\n######################\nOutput:\n(\"entity\"<|>开封<|>GEO<|>地点，中国历史上的城市)\n##\n(\"entity\"<|>1126年<|>DATE<|>年份，记录事件发生的时间)\n##\n(\"entity\"<|>1126年5月<|>DATE<|>具体时间，事件发生的月份)\n##\n(\"entity\"<|>白居易<|>PERSON<|>唐代著名诗人，书写《楞严经》的作者)\n##\n(\"relationship\"<|>开封<|>1126年5月<|>在1126年5月，事件发生在开封<|>2)\n##\n(\"relationship\"<|>夫妻<|>白居易<|>夫妻欣赏了白居易所书写的《楞严经》<|>2)\n##\n(\"relationship\"<|>1126年5月<|>白居易<|>在1126年5月，夫妻共赏白居易的作品<|>2)\n<|COMPLETE|>\n\n######################\n案例 2:\nEntity_types: ORGANIZATION,GEO,PERSON,DATE\nText:\n莱州（掖县）（1121年8月18日 - 1123年，38-40岁，作品：2）\n1121-1123年，38-40岁\n1121年8月18日　到莱州，有诗。作品：《感怀》\n感怀（1121年8月18日） 宋 · 李清照\n寒窗败几无书史，公路可怜竟至此。\n青州从事孔方君，终日纷纷喜生事。\n作诗谢绝聊闭门，燕寝凝香有佳思。\n静中我乃得知交，乌有先生子虚子⑴。\n⑴ 以上明郦琥《彤管遣编》续集卷一七\n\n1122年　夫妇居莱州，明诚屡得石刻碑铭，清照有诗。作品：《晓梦》\n晓梦（1122年） 宋 · 李清照\n晓梦随疏钟，飘然蹑云霞。\n因缘安期生，邂逅萼绿华。\n秋风正无赖，吹尽玉井花。\n共看藕如船，同食枣如瓜。\n翩翩坐上客，意妙语亦佳。\n嘲辞斗诡辨，活火分新茶。\n虽非助帝功，其乐莫可涯。\n人生能如此，何必归故家。\n起来敛衣坐，掩耳厌喧哗。\n心知不可见，念念犹咨嗟。\n\n1123年　居莱州，于静治堂助明诚整理《金石录》\n\n######################\nOutput:\n(\"entity\"<|>莱州<|>GEO<|>李清照在1121年至1123年间居住的地方)\n##\n(\"entity\"<|>掖县<|>GEO<|>莱州的古称)\n##\n(\"entity\"<|>青州<|>GEO<|>李清照提到的地点之一)\n##\n(\"entity\"<|>玉井<|>GEO<|>《晓梦》诗中提到的地点)\n##\n(\"entity\"<|>静治堂<|>GEO<|>李清照与明诚整理《金石录》的地方)\n##\n(\"entity\"<|>李清照<|>PERSON<|>宋代著名女词人，创作了《感怀》和《晓梦》)\n##\n(\"entity\"<|>明诚<|>PERSON<|>李清照的丈夫，与她共同整理《金石录》)\n##\n(\"entity\"<|>安期生<|>PERSON<|>《晓梦》诗中提到的仙人)\n##\n(\"entity\"<|>萼绿华<|>PERSON<|>《晓梦》诗中提到的仙女)\n##\n(\"entity\"<|>郦琥<|>PERSON<|>明代学者，其著作《彤管遣编》收录了李清照的作品)\n##\n(\"entity\"<|>乌有先生<|>PERSON<|>李清照诗中虚构的人物)\n##\n(\"entity\"<|>子虚子<|>PERSON<|>李清照诗中虚构的人物)\n##\n(\"entity\"<|>《感怀》<|>ORGANIZATION<|>李清照于1121年创作的诗作)\n##\n(\"entity\"<|>《晓梦》<|>ORGANIZATION<|>李清照于1122年创作的诗作)\n##\n(\"entity\"<|>《金石录》<|>ORGANIZATION<|>李清照与明诚整理的金石学著作)\n##\n(\"entity\"<|>1121年8月18日<|>TIME<|>李清照到达莱州并创作《感怀》的时间)\n##\n(\"entity\"<|>1121年<|>TIME<|>李清照开始在莱州居住的时间)\n##\n(\"entity\"<|>1122年<|>TIME<|>李清照创作《晓梦》的时间)\n##\n(\"entity\"<|>1123年<|>TIME<|>李清照与明诚在静治堂整理《金石录》的时间)\n##\n(\"relationship\"<|>李清照<|>莱州<|>李清照在莱州居住并创作了多首诗作<|>2)\n##\n(\"relationship\"<|>李清照<|>明诚<|>李清照与明诚共同整理《金石录》<|>2)\n##\n(\"relationship\"<|>李清照<|>《感怀》<|>李清照于1121年8月18日创作了《感怀》<|>2)\n##\n(\"relationship\"<|>李清照<|>《晓梦》<|>李清照于1122年创作了《晓梦》<|>2)\n##\n(\"relationship\"<|>李清照<|>静治堂<|>李清照在静治堂协助明诚整理《金石录》<|>2)\n##\n(\"relationship\"<|>明诚<|>《金石录》<|>明诚在莱州整理《金石录》<|>2)\n##\n(\"relationship\"<|>李清照<|>乌有先生<|>李清照在《感怀》中提到了乌有先生<|>2)\n##\n(\"relationship\"<|>李清照<|>子虚子<|>李清照在《感怀》中提到了子虚子<|>2)\n##\n(\"relationship\"<|>李清照<|>安期生<|>李清照在《晓梦》中提到了安期生<|>2)\n##\n(\"relationship\"<|>李清照<|>萼绿华<|>李清照在《晓梦》中提到了萼绿华<|>2)\n##\n(\"relationship\"<|>李清照<|>郦琥<|>郦琥在其著作《彤管遣编》中收录了李清照的作品<|>2)\n##\n(\"relationship\"<|>李清照<|>1121年8月18日<|>李清照于1121年8月18日到达莱州并创作了《感怀》<|>2)\n##\n(\"relationship\"<|>李清照<|>1122年<|>李清照于1122年创作了《晓梦》<|>2)\n##\n(\"relationship\"<|>李清照<|>1123年<|>李清照于1123年在静治堂整理《金石录》<|>2)\n<|COMPLETE|>\n\n######################\n-真实数据-\n######################\nEntity_types: ORGANIZATION,PERSON,GEO,EVENT,DATE\nText: 7月　金人立刘豫为帝，清照作诗斥伪齐。作品：《咏史》\n\n绍兴（1131年3月 - 1131年8月4日，48岁）\n1131年，48岁\n1131年3月　复追随高宗赴越州，文物被盗五簏。\n1131年8月4日　弟李迒转一官，权工部侍郎。\n######################\n输出:",
        "kwargs": {}
    }
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 166, in _execute_llm\n    completion = await self._client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1727, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1849, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1543, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1644, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': '当前分组上游负载已饱和，请稍后再试 (request id: 20250328164517565361306BiNdpqZB)', 'type': 'new_api_error', 'param': '', 'code': 'get_channel_failed'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 74, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 40, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"D:\\Miniconda3\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 166, in _execute_llm\n    completion = await self._client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1727, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1849, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1543, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1644, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.InternalServerError: Error code: 500 - {'error': {'message': '当前分组上游负载已饱和，请稍后再试 (request id: 20250328164324616925481BqZviKPE) (request id: 20250328164536250029003ooLsOOM4)', 'type': 'new_api_error', 'param': '', 'code': 'no_more_channels_available'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 74, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 40, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"D:\\Miniconda3\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 166, in _execute_llm\n    completion = await self._client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1727, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1849, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1543, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1644, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': '当前分组上游负载已饱和，请稍后再试 (request id: 20250328164539886208117889JXGWL)', 'type': 'new_api_error', 'param': '', 'code': 'get_channel_failed'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 74, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 40, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"D:\\Miniconda3\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 166, in _execute_llm\n    completion = await self._client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1727, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1849, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1543, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1644, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.InternalServerError: Error code: 500 - {'error': {'message': '当前分组上游负载已饱和，请稍后再试 (request id: 202503281643319359165787YjEjept) (request id: 20250328164543562573076cXZv3590)', 'type': 'new_api_error', 'param': '', 'code': 'no_more_channels_available'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 74, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 40, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"D:\\Miniconda3\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 166, in _execute_llm\n    completion = await self._client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1727, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1849, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1543, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1644, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': '当前分组上游负载已饱和，请稍后再试 (request id: 20250328164539695172100xfe0F55K)', 'type': 'new_api_error', 'param': '', 'code': 'get_channel_failed'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 74, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 40, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"D:\\Miniconda3\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 166, in _execute_llm\n    completion = await self._client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1727, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1849, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1543, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1629, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1644, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': '当前分组上游负载已饱和，请稍后再试 (request id: 20250328164550947252514hWEzX7K1)', 'type': 'new_api_error', 'param': '', 'code': 'get_channel_failed'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 74, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 40, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"D:\\Miniconda3\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"D:\\Miniconda3\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"D:\\Miniconda3\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
